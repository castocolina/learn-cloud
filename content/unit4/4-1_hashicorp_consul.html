<div class="container">
    <div class="page-header">
        <h1>4.1: HashiCorp Consul</h1>
    </div>

    <p class="lead">Welcome to Unit 4. As we build more complex microservice architectures, new challenges arise. How do services find each other in a dynamic environment where IPs and ports are constantly changing? How do we manage configuration centrally? How do we secure the traffic between services? <a href="https://www.consul.io/" target="_blank">HashiCorp Consul</a> is a comprehensive service networking solution that addresses these challenges through service discovery, a distributed key-value store, and a built-in service mesh.</p>

    <hr/>

    <!-- Architecture -->
    <h2><i class="bi bi-diagram-3"></i> 1. Consul Deployment Architecture</h2>
    <p>
        A Consul cluster consists of <strong>agents</strong> running in either <strong>server</strong> or <strong>client</strong> mode.
    </p>
    <ul>
        <li><strong>Servers:</strong> A small number of nodes (typically 3 or 5 for high availability) that form the "brain" of the cluster. They store all the state (service catalog, KV data, etc.) and use the <a href="https://raft.github.io/" target="_blank">Raft consensus algorithm</a> to ensure consistency.</li>
        <li><strong>Clients:</strong> Lightweight agents that run on every node where your services are running. They act as a local proxy to the Consul servers, register local services, and perform health checks.</li>
    </ul>

    <div class="text-center my-4">
        <pre class="mermaid">
            graph TD
                subgraph "Datacenter (dc1)"
                    subgraph "Consul Servers (HA Cluster)"
                        S1["Server 1"] <--> S2["Server 2"];
                        S2 <--> S3["Server 3"];
                        S3 <--> S1;
                    end
                    subgraph "Node A"
                        ClientA["Consul Client"] --> S1;
                        Service1["Service A"] -- "Registers with" --> ClientA;
                    end
                    subgraph "Node B"
                        ClientB["Consul Client"] --> S2;
                        Service2["Service B"] -- "Registers with" --> ClientB;
                    end
                    subgraph "Node C"
                        ClientC["Consul Client"] --> S3;
                        Service3["Service C"] -- "Registers with" --> ClientC;
                    end
                end
        </pre>
        <small class="text-muted">Diagram: High-Availability Consul Cluster Architecture</small>
    </div>

    <h4>Setup: A Simple Dev Cluster with Docker</h4>
    <p>For development, you can spin up a single Consul server in dev mode.</p>
    <pre><code class="language-bash">
docker run -d --name=consul -p 8500:8500 -p 8600:8600/udp hashicorp/consul:1.18 agent -server -ui -client=0.0.0.0 -bootstrap-expect=1
    </code></pre>
    <p>This command starts a Consul server and exposes the UI and API on port <code>8500</code> and the DNS interface on port <code>8600</code>. You can access the UI at <a href="http://localhost:8500" target="_blank">http://localhost:8500</a>.</p>

    <hr/>

    <!-- Service Discovery -->
    <h2><i class="bi bi-search"></i> 2. Service Discovery</h2>
    <p>
        This is Consul's core feature. Instead of hardcoding IP addresses, services query Consul to find the location of their dependencies. This is critical in modern environments where services are ephemeral.
    </p>
    <p>The workflow is simple:</p>
    <ol>
        <li>A service instance starts up on a node.</li>
        <li>The local Consul client registers the service with the Consul catalog, providing its name, IP, port, and a health check.</li>
        <li>When another service wants to connect, it queries Consul for a healthy instance of the desired service.</li>
        <li>Consul provides the IP and port of a healthy instance, filtering out any that are failing their health checks.</li>
    </ol>

    <h4>Querying for Services</h4>
    <p>Services can be discovered via two main interfaces:</p>
    <ul>
        <li><strong>DNS Interface:</strong> Consul provides a DNS server that allows you to resolve services using special names like <code>my-service.service.consul</code>. This often requires no application code changes.</li>
        <li><strong>HTTP API:</strong> A more flexible RESTful API for querying services, health, and other Consul data.</li>
    </ul>

    <div class="text-center my-4">
        <pre class="mermaid">
            graph TD
                A["Service A"] -- "1. Where is Service B?" --> ConsulDNS["Consul DNS/HTTP API"];
                ConsulDNS -- "2. Checks Catalog" --> ConsulCatalog["Service Catalog (Healthy Nodes)"];
                ConsulCatalog -- "3. B is at 10.1.2.3:8080" --> ConsulDNS;
                ConsulDNS -- "4. Returns address" --> A;
                A -- "5. Connects directly" --> B["Service B (10.1.2.3:8080)"];
        </pre>
        <small class="text-muted">Diagram: Consul Service Discovery Flow</small>
    </div>

    <hr/>

    <!-- KV Store -->
    <h2><i class="bi bi-key"></i> 3. Distributed Key-Value (KV) Store</h2>
    <p>
        Consul includes a hierarchical key-value store for managing dynamic configuration, feature flags, and service coordination. It can be accessed via the HTTP API or CLI.
    </p>
    <pre><code class="language-bash">
# Set a value for a feature flag
consul kv put features/new-api true

# Get the value
consul kv get features/new-api
# Output: true

# Get with detailed information
consul kv get -detailed features/new-api
    </code></pre>
    <p>Applications can query this KV store at startup or runtime to dynamically change their behavior without needing a redeploy.</p>

    <hr/>

    <!-- Consul Connect -->
    <h2><i class="bi bi-shield-check"></i> 4. Consul Connect for Service Mesh</h2>
    <p>
        A <strong>service mesh</strong> is a dedicated infrastructure layer for making service-to-service communication safe, fast, and reliable. <strong>Consul Connect</strong> is Consul's built-in service mesh solution. Its primary feature is providing universal <strong>mutual TLS (mTLS)</strong> for all traffic.
    </p>
    <p>How it works:</p>
    <ol>
        <li>You register a service with Consul and enable it for Connect.</li>
        <li>Consul automatically generates and distributes TLS certificates for that service.</li>
        <li>A lightweight <strong>sidecar proxy</strong> (like Envoy) is deployed next to your service instance.</li>
        <li>All inbound and outbound traffic for your service flows through the proxy. The proxy automatically encrypts and decrypts traffic using mTLS, verifying the identity of other services based on their certificates.</li>
    </ol>
    <p>This secures all communication within your cluster automatically, without requiring any changes to your application code.</p>

    <div class="text-center my-4">
        <pre class="mermaid">
            graph TD
                subgraph "Node A"
                    ServiceA["Service A"] <--> ProxyA["Sidecar Proxy"];
                end
                subgraph "Node B"
                    ServiceB["Service B"] <--> ProxyB["Sidecar Proxy"];
                end

                ProxyA -- "mTLS Encrypted Traffic" <--> ProxyB;
                Consul["Consul Server"] -- "Manages Certificates & Policies" --> ProxyA;
                Consul -- "Manages Certificates & Policies" --> ProxyB;
        </pre>
        <small class="text-muted">Diagram: Service-to-Service Communication with Consul Connect</small>
    </div>

</div>
